{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the benchmark by Matthew Rocklin (https://matthewrocklin.com/blog/work/2017/07/03/scaling), but expanded to also look at the difference between cores and threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, wait\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(func, client):\n",
    "\n",
    "    client = client or default_client()\n",
    "    \n",
    "    workers = len(client.ncores())\n",
    "    threads = list(client.ncores().values())[0]\n",
    "    n = sum(client.ncores().values())\n",
    "    \n",
    "    coroutine = func(n)\n",
    "\n",
    "    name, unit, numerator = next(coroutine)\n",
    "    out = []\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            next_name, next_unit, next_numerator = next(coroutine)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        finally:\n",
    "            end = time.time()\n",
    "            record = {'name': name, \n",
    "                      'duration': end - start, \n",
    "                      'unit': unit + '/s', \n",
    "                      'rate': numerator / ((end - start) + 1e-10), \n",
    "                      'n': n,\n",
    "                      'workers': workers,\n",
    "                      'threads': threads,\n",
    "                      'collection': func.__name__}\n",
    "            out.append(record)\n",
    "        name = next_name\n",
    "        unit = next_unit\n",
    "        numerator = next_numerator\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import time\n",
    "\n",
    "def slowinc(x, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return x + 1\n",
    "\n",
    "def slowadd(x, y, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return x + y\n",
    "\n",
    "def slowsum(L, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return sum(L)\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def tasks(n):\n",
    "    yield 'task map fast tasks', 'tasks', n * 200\n",
    "    \n",
    "    futures = client.map(inc, range(n * 200))\n",
    "    wait(futures)\n",
    "    \n",
    "    yield 'task map 100ms tasks', 'tasks', n * 100\n",
    "\n",
    "    futures = client.map(slowinc, range(100 * n))\n",
    "    wait(futures)\n",
    "        \n",
    "    yield 'task map 1s tasks', 'tasks', n * 4\n",
    "\n",
    "    futures = client.map(slowinc, range(4 * n), delay=1)\n",
    "    wait(futures)\n",
    "\n",
    "    yield 'tree reduction fast tasks', 'tasks', 2**7 * n\n",
    "    \n",
    "    from dask import delayed\n",
    "\n",
    "    L = range(2**7 * n)\n",
    "    while len(L) > 1:\n",
    "        L = list(map(delayed(operator.add), L[0::2], L[1::2]))\n",
    "\n",
    "    L[0].compute()\n",
    "    \n",
    "    yield 'tree reduction 100ms tasks', 'tasks', 2**6 * n * 2\n",
    "    \n",
    "    from dask import delayed\n",
    "\n",
    "    L = range(2**6 * n)\n",
    "    while len(L) > 1:\n",
    "        L = list(map(delayed(slowadd), L[0::2], L[1::2]))\n",
    "\n",
    "    L[0].compute()\n",
    "    \n",
    "    yield 'sequential', 'tasks', 100\n",
    "\n",
    "    x = 1\n",
    "\n",
    "    for i in range(100):\n",
    "        x = delayed(inc)(x)\n",
    "        \n",
    "    x.compute()\n",
    "    \n",
    "    yield 'dynamic tree reduction fast tasks', 'tasks', 100 * n\n",
    "    \n",
    "    from dask.distributed import as_completed\n",
    "    futures = client.map(inc, range(n * 100))\n",
    "    \n",
    "    pool = as_completed(futures)\n",
    "    batches = pool.batches()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batches)\n",
    "            if len(batch) == 1:\n",
    "                batch += next(batches)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        future = client.submit(sum, batch)\n",
    "        pool.add(future)\n",
    "        \n",
    "    yield 'dynamic tree reduction 100ms tasks', 'tasks', 100 * n\n",
    "    \n",
    "    from dask.distributed import as_completed\n",
    "    futures = client.map(slowinc, range(n * 20))\n",
    "    \n",
    "    pool = as_completed(futures)\n",
    "    batches = pool.batches()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batches)\n",
    "            if len(batch) == 1:\n",
    "                batch += next(batches)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        future = client.submit(slowsum, batch)\n",
    "        pool.add(future)\n",
    "\n",
    "        \n",
    "    yield 'nearest neighbor fast tasks', 'tasks', 100 * n * 2\n",
    "    \n",
    "    L = range(100 * n)\n",
    "    L = client.map(operator.add, L[:-1], L[1:])\n",
    "    L = client.map(operator.add, L[:-1], L[1:])\n",
    "    wait(L)\n",
    "    \n",
    "    yield 'nearest neighbor 100ms tasks', 'tasks', 20 * n * 2\n",
    "    \n",
    "    L = range(20 * n)\n",
    "    L = client.map(slowadd, L[:-1], L[1:])\n",
    "    L = client.map(slowadd, L[:-1], L[1:])\n",
    "    wait(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays(n):\n",
    "    import dask.array as da\n",
    "    N = int(2000 * math.sqrt(n))\n",
    "    x = da.random.randint(0, 10000, size=(N, N), chunks=(2000, 2000))\n",
    "    \n",
    "    yield 'create random', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x = x.persist()\n",
    "    wait(x)\n",
    "    \n",
    "    yield 'blockwise 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_blocks(slowinc, dtype=x.dtype).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'random access', 'bytes', 8\n",
    "    \n",
    "    x[1234, 4567].compute()\n",
    "   \n",
    "    yield 'reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x.std().compute()\n",
    "    \n",
    "    yield 'reduction along axis', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x.std(axis=0).compute()\n",
    "    \n",
    "    yield 'elementwise computation', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = da.sin(x) ** 2 + da.cos(x) ** 2\n",
    "    y = y.persist()\n",
    "    wait(y)    \n",
    "    \n",
    "    yield 'rechunk small', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.rechunk((20000, 200)).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'rechunk large', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = y.rechunk((200, 20000)).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'transpose addition', 'MB', x.nbytes / 1e6\n",
    "    y = x + x.T\n",
    "    y = y.persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'nearest neighbor fast tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_overlap(inc, depth=1).persist()\n",
    "    wait(y)   \n",
    "        \n",
    "    yield 'nearest neighbor 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_overlap(slowinc, depth=1, delay=0.1).persist()\n",
    "    wait(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframes(n):\n",
    "    import dask.array as da\n",
    "    import dask.dataframe as dd\n",
    "    N = 1000000 * math.sqrt(n)\n",
    "    \n",
    "    x = da.random.randint(0, 10000, size=(N, 10), chunks=(2000000, 10))\n",
    "\n",
    "    \n",
    "    yield 'create random', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df = dd.from_dask_array(x).persist()\n",
    "    wait(df)\n",
    "    \n",
    "    yield 'blockwise 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.map_partitions(slowinc, meta=df).persist())\n",
    "    \n",
    "    yield 'arithmetic', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = (df[0] + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'random access', 'bytes', 8\n",
    "    \n",
    "    df.loc[123456].compute()\n",
    "    \n",
    "    yield 'dataframe reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.std().compute()\n",
    "    \n",
    "    yield 'series reduction', 'MB', x.nbytes / 1e6 / 10\n",
    "    \n",
    "    df[3].std().compute()\n",
    "    \n",
    "    yield 'groupby reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.groupby(0)[1].mean().compute()\n",
    "    \n",
    "    yield 'groupby apply (full shuffle)', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.groupby(0).apply(len, meta=('x', 'f8')).compute()\n",
    "    \n",
    "    yield 'set index (full shuffle)', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.set_index(1).persist())\n",
    "    \n",
    "    yield 'rolling aggregations', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.rolling(5).mean().persist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started cluster with 32 workers with 4 threads\n",
      "Dashboard: http://127.0.0.1:42827/status\n",
      "Iteration 1: tasks\n",
      "Iteration 1: arrays\n",
      "Iteration 1: dataframes\n",
      "Iteration 2: tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: arrays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: dataframes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: tasks\n",
      "Iteration 3: arrays\n",
      "Iteration 3: dataframes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ef0f76036b07>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(func, client)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mnext_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_numerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-56ee460724d0>\u001b[0m in \u001b[0;36mdataframes\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dask_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0;34m'blockwise 100ms tasks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m   4018\u001b[0m     \"\"\"\n\u001b[1;32m   4019\u001b[0m     \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_when\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_when\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             return sync(\n\u001b[0;32m--> 757\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m             )\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cores_list = [2, 4, 8, 16, 32, 64]\n",
    "cores_list = [128, 64, 32, 16, 8, 4]\n",
    "threads_list = [4]\n",
    "\n",
    "L = []\n",
    "for cores in cores_list:\n",
    "    for threads in threads_list:\n",
    "        if threads > cores:\n",
    "            break\n",
    "\n",
    "        if (cores / threads).is_integer():\n",
    "            workers = int(cores / threads)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        cluster = LocalCluster(\n",
    "            n_workers=workers,\n",
    "            threads_per_worker=threads,\n",
    "            dashboard_address=':0'\n",
    "        )\n",
    "\n",
    "        print(f\"Started cluster with {workers} workers with {threads} threads\")\n",
    "        print(f\"Dashboard: {cluster.dashboard_link}\")\n",
    "        with Client(cluster) as client:\n",
    "            for i in range(3):\n",
    "                for func in [tasks, arrays, dataframes]:\n",
    "                    print(f\"Iteration {i+1}: {func.__name__}\")\n",
    "                    df = run(func, client=client)\n",
    "                    L.append(df)\n",
    "            print('Computation complete! Stopping workers...')\n",
    "\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6becd2769a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pda' is not defined"
     ]
    }
   ],
   "source": [
    "ddf = pda.concat(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf.groupby(['collection', 'name', 'n', 'workers', 'threads', 'unit']).median()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scaling-data-cores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import Row, Column, gridplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(part, axis_type='linear'):\n",
    "    row = part.iloc[0]\n",
    "    title = (row['collection'] + ': ' + row['name']).title().replace('0M', '0m').replace('1S', '1s')\n",
    "\n",
    "    fig = figure(title=title, sizing_mode='scale_width', x_axis_type=axis_type, y_axis_type=axis_type)\n",
    "    fig.line(x=part.threads, y=part.rate)\n",
    "    fig.circle(x=part.threads, y=part.rate)\n",
    "    fig.xaxis.axis_label = 'threads'\n",
    "    fig.yaxis.axis_label = row['unit']\n",
    "    fig.x_range.start = 0\n",
    "    fig.y_range.start = 0\n",
    "\n",
    "    # Add in perfect scaling line\n",
    "#     y_end = fig.y_range.end\n",
    "#     mn = part.n.min()\n",
    "#     mx = part.n.max()\n",
    "#     slope = part[part.n == mn].iloc[0]['rate'] / mn\n",
    "#     fig.line(x=[0, mx], y=[0, slope * mx], color='gray', line_dash='dashed')\n",
    "#     fig.y_range.end = part.rate.max()\n",
    "    \n",
    "    fig.xaxis.ticker = part.threads\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_plot(part, axis_type='linear'):\n",
    "    row = part.iloc[0]\n",
    "    title = (row['collection'] + ': ' + row['name']).title().replace('0M', '0m').replace('1S', '1s')\n",
    "\n",
    "    fig = figure(title=title, sizing_mode='scale_width', x_axis_type=axis_type, y_axis_type=axis_type)\n",
    "    fig.line(x=part.n, y=part.rate)\n",
    "    fig.circle(x=part.n, y=part.rate)\n",
    "    fig.xaxis.axis_label = 'cores'\n",
    "    fig.yaxis.axis_label = row['unit']\n",
    "    fig.x_range.start = 0\n",
    "    fig.y_range.start = 0\n",
    "\n",
    "    # Add in perfect scaling line\n",
    "    y_end = fig.y_range.end\n",
    "    mn = part.n.min()\n",
    "    mx = part.n.max()\n",
    "    slope = part[part.n == mn].iloc[0]['rate'] / mn\n",
    "    fig.line(x=[0, mx], y=[0, slope * mx], color='gray', line_dash='dashed')\n",
    "    fig.y_range.end = part.rate.max()\n",
    "    \n",
    "    fig.xaxis.ticker = part.n\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scaling-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['collection', 'name'])['collection', 'name', 'n', 'rate', 'unit', 'threads'].apply(comparison_plot)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['task map 1s tasks', 'task map 100ms tasks', 'task map fast tasks',\n",
    "         'tree reduction 100ms tasks', 'tree reduction fast tasks', 'sequential',\n",
    "         'nearest neighbor 100ms tasks', 'nearest neighbor fast tasks', \n",
    "         'dynamic tree reduction 100ms tasks', 'dynamic tree reduction fast tasks']\n",
    "from toolz import partition_all\n",
    "L = df2.loc['tasks'].loc[names].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['create random', 'blockwise 100ms tasks', 'elementwise computation', 'reduction', \n",
    "         'reduction along axis', 'random access', 'transpose addition', 'rechunk large', \n",
    "         'nearest neighbor fast tasks', 'nearest neighbor 100ms tasks']\n",
    "from toolz import partition_all\n",
    "L = df2.loc['arrays'].loc[names].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['task map 1s tasks', 'task map 100ms tasks', 'task map fast tasks',\n",
    "         'tree reduction 100ms tasks', 'tree reduction fast tasks', 'sequential',\n",
    "         'nearest neighbor 100ms tasks', 'nearest neighbor fast tasks', \n",
    "         'dynamic tree reduction 100ms tasks', 'dynamic tree reduction fast tasks']\n",
    "L = df2.loc['dataframes'].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(file, method):\n",
    "    if method == 'dask':\n",
    "        t = %timeit -o -n 1 -r 2 dd.read_csv(file + \".csv\")\n",
    "        return t.timings\n",
    "    elif method == 'dask_compute':\n",
    "        t = %timeit -o -n 1 -r 2 dd.read_csv(file + \".csv\").compute()\n",
    "        return t.timings\n",
    "    elif method == 'pandas':\n",
    "        t = %timeit -o -n 1 -r 2 pd.read_csv(file + \".csv\")\n",
    "        return t.timings\n",
    "    else:\n",
    "        return 'Choose an available method'\n",
    "\n",
    "methods = ['dask', 'dask_compute', 'pandas']\n",
    "datasets = ['random', 'random_0.5', 'random_0.1']\n",
    "\n",
    "# benchmark\n",
    "result = {method: [load_df(dataset,method) for dataset in datasets] for method in methods}\n",
    "\n",
    "# Make dataframe\n",
    "df = pd.DataFrame.from_dict(result, orient='index', columns=datasets).reset_index()\n",
    "df.columns = ['method', 'random', 'random_0.5', 'random_0.1']\n",
    "\n",
    "# Make separate columns into one row\n",
    "df = df.melt(id_vars='method', var_name='dataset', value_vars=datasets, value_name='average')\n",
    "\n",
    "# Explode every measurement into separate rows\n",
    "df = df.explode('average')\n",
    "\n",
    "# Plot that shit\n",
    "sns.catplot(data=df, x='dataset', y='average', hue='method', kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
