{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the benchmark by Matthew Rocklin (https://matthewrocklin.com/blog/work/2017/07/03/scaling), but expanded to also look at the difference between cores and threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dustin-xomnia\\Miniconda3\\envs\\dask\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster, wait\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(func, client):\n",
    "\n",
    "    client = client or default_client()\n",
    "    \n",
    "    workers = len(client.ncores())\n",
    "    threads = list(client.ncores().values())[0]\n",
    "    n = sum(client.ncores().values())\n",
    "    \n",
    "    coroutine = func(n)\n",
    "\n",
    "    name, unit, numerator = next(coroutine)\n",
    "    out = []\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            next_name, next_unit, next_numerator = next(coroutine)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        finally:\n",
    "            end = time.time()\n",
    "            record = {'name': name, \n",
    "                      'duration': end - start, \n",
    "                      'unit': unit + '/s', \n",
    "                      'rate': numerator / ((end - start) + 1e-10), \n",
    "                      'n': n,\n",
    "                      'workers': workers,\n",
    "                      'threads': threads,\n",
    "                      'collection': func.__name__}\n",
    "            out.append(record)\n",
    "        name = next_name\n",
    "        unit = next_unit\n",
    "        numerator = next_numerator\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import time\n",
    "\n",
    "def slowinc(x, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return x + 1\n",
    "\n",
    "def slowadd(x, y, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return x + y\n",
    "\n",
    "def slowsum(L, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return sum(L)\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def tasks(n):\n",
    "    yield 'task map fast tasks', 'tasks', n * 200\n",
    "    \n",
    "    futures = client.map(inc, range(n * 200))\n",
    "    wait(futures)\n",
    "    \n",
    "    yield 'task map 100ms tasks', 'tasks', n * 100\n",
    "\n",
    "    futures = client.map(slowinc, range(100 * n))\n",
    "    wait(futures)\n",
    "        \n",
    "    yield 'task map 1s tasks', 'tasks', n * 4\n",
    "\n",
    "    futures = client.map(slowinc, range(4 * n), delay=1)\n",
    "    wait(futures)\n",
    "\n",
    "    yield 'tree reduction fast tasks', 'tasks', 2**7 * n\n",
    "    \n",
    "    from dask import delayed\n",
    "\n",
    "    L = range(2**7 * n)\n",
    "    while len(L) > 1:\n",
    "        L = list(map(delayed(operator.add), L[0::2], L[1::2]))\n",
    "\n",
    "    L[0].compute()\n",
    "    \n",
    "    yield 'tree reduction 100ms tasks', 'tasks', 2**6 * n * 2\n",
    "    \n",
    "    from dask import delayed\n",
    "\n",
    "    L = range(2**6 * n)\n",
    "    while len(L) > 1:\n",
    "        L = list(map(delayed(slowadd), L[0::2], L[1::2]))\n",
    "\n",
    "    L[0].compute()\n",
    "    \n",
    "    yield 'sequential', 'tasks', 100\n",
    "\n",
    "    x = 1\n",
    "\n",
    "    for i in range(100):\n",
    "        x = delayed(inc)(x)\n",
    "        \n",
    "    x.compute()\n",
    "    \n",
    "    yield 'dynamic tree reduction fast tasks', 'tasks', 100 * n\n",
    "    \n",
    "    from dask.distributed import as_completed\n",
    "    futures = client.map(inc, range(n * 100))\n",
    "    \n",
    "    pool = as_completed(futures)\n",
    "    batches = pool.batches()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batches)\n",
    "            if len(batch) == 1:\n",
    "                batch += next(batches)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        future = client.submit(sum, batch)\n",
    "        pool.add(future)\n",
    "        \n",
    "    yield 'dynamic tree reduction 100ms tasks', 'tasks', 100 * n\n",
    "    \n",
    "    from dask.distributed import as_completed\n",
    "    futures = client.map(slowinc, range(n * 20))\n",
    "    \n",
    "    pool = as_completed(futures)\n",
    "    batches = pool.batches()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batches)\n",
    "            if len(batch) == 1:\n",
    "                batch += next(batches)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        future = client.submit(slowsum, batch)\n",
    "        pool.add(future)\n",
    "\n",
    "        \n",
    "    yield 'nearest neighbor fast tasks', 'tasks', 100 * n * 2\n",
    "    \n",
    "    L = range(100 * n)\n",
    "    L = client.map(operator.add, L[:-1], L[1:])\n",
    "    L = client.map(operator.add, L[:-1], L[1:])\n",
    "    wait(L)\n",
    "    \n",
    "    yield 'nearest neighbor 100ms tasks', 'tasks', 20 * n * 2\n",
    "    \n",
    "    L = range(20 * n)\n",
    "    L = client.map(slowadd, L[:-1], L[1:])\n",
    "    L = client.map(slowadd, L[:-1], L[1:])\n",
    "    wait(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays(n):\n",
    "    import dask.array as da\n",
    "    N = int(2000 * math.sqrt(n))\n",
    "    x = da.random.randint(0, 10000, size=(N, N), chunks=(2000, 2000))\n",
    "    \n",
    "    yield 'create random', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x = x.persist()\n",
    "    wait(x)\n",
    "    \n",
    "    yield 'blockwise 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_blocks(slowinc, dtype=x.dtype).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'random access', 'bytes', 8\n",
    "    \n",
    "    x[1234, 4567].compute()\n",
    "   \n",
    "    yield 'reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x.std().compute()\n",
    "    \n",
    "    yield 'reduction along axis', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x.std(axis=0).compute()\n",
    "    \n",
    "    yield 'elementwise computation', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = da.sin(x) ** 2 + da.cos(x) ** 2\n",
    "    y = y.persist()\n",
    "    wait(y)    \n",
    "    \n",
    "    yield 'rechunk small', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.rechunk((20000, 200)).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'rechunk large', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = y.rechunk((200, 20000)).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'transpose addition', 'MB', x.nbytes / 1e6\n",
    "    y = x + x.T\n",
    "    y = y.persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'nearest neighbor fast tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_overlap(inc, depth=1).persist()\n",
    "    wait(y)   \n",
    "        \n",
    "    yield 'nearest neighbor 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_overlap(slowinc, depth=1, delay=0.1).persist()\n",
    "    wait(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframes(n):\n",
    "    import dask.array as da\n",
    "    import dask.dataframe as dd\n",
    "    N = int(1000000 * math.sqrt(n))\n",
    "    \n",
    "    x = da.random.randint(0, 10000, size=(N, 10), chunks=(2000000, 10))\n",
    "\n",
    "    \n",
    "    yield 'create random', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df = dd.from_dask_array(x).persist()\n",
    "    wait(df)\n",
    "    \n",
    "    yield 'blockwise 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.map_partitions(slowinc, meta=df).persist())\n",
    "    \n",
    "    yield 'arithmetic', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = (df[0] + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'random access', 'bytes', 8\n",
    "    \n",
    "    df.loc[123456].compute()\n",
    "    \n",
    "    yield 'dataframe reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.std().compute()\n",
    "    \n",
    "    yield 'series reduction', 'MB', x.nbytes / 1e6 / 10\n",
    "    \n",
    "    df[3].std().compute()\n",
    "    \n",
    "    yield 'groupby reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.groupby(0)[1].mean().compute()\n",
    "    \n",
    "    yield 'groupby apply (full shuffle)', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.groupby(0).apply(len, meta=('x', 'f8')).compute()\n",
    "    \n",
    "    yield 'set index (full shuffle)', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.set_index(1).persist())\n",
    "    \n",
    "    yield 'rolling aggregations', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.rolling(5).mean().persist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started cluster with 8 workers with 4 threads\n",
      "Dashboard: http://127.0.0.1:53257/status\n",
      "Iteration 1: tasks\n",
      "Iteration 1: arrays\n",
      "Iteration 1: dataframes\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape can only contain integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ef0f76036b07>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(func, client)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcoroutine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-2265832e7817>\u001b[0m in \u001b[0;36mdataframes\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000000\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\dask\\lib\\site-packages\\dask\\array\\random.py\u001b[0m in \u001b[0;36mrandint\u001b[1;34m(self, low, high, size, chunks, dtype)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdoc_wraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"l\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"randint\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdoc_wraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_integers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\dask\\lib\\site-packages\\dask\\array\\random.py\u001b[0m in \u001b[0;36m_wrap\u001b[1;34m(self, funcname, *args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# ideally would use dtype here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         )\n\u001b[0;32m    100\u001b[0m         \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslices_from_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\dask\\lib\\site-packages\\dask\\array\\core.py\u001b[0m in \u001b[0;36mnormalize_chunks\u001b[1;34m(chunks, shape, limit, dtype, previous_chunks)\u001b[0m\n\u001b[0;32m   2445\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m             ),\n\u001b[1;32m-> 2447\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2448\u001b[0m         )\n\u001b[0;32m   2449\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\dask\\lib\\site-packages\\dask\\array\\core.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2443\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2444\u001b[0m                 \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2445\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2446\u001b[0m             ),\n\u001b[0;32m   2447\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\dask\\lib\\site-packages\\dask\\array\\core.py\u001b[0m in \u001b[0;36mblockdims_from_blockshape\u001b[1;34m(shape, chunks)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chunks can only contain integers.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shape can only contain integers.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shape can only contain integers."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cores_list = [2, 4, 8, 16, 32, 64]\n",
    "cores_list = [32, 16, 8, 4]\n",
    "threads_list = [4]\n",
    "\n",
    "L = []\n",
    "for cores in cores_list:\n",
    "    for threads in threads_list:\n",
    "        if threads > cores:\n",
    "            break\n",
    "\n",
    "        if (cores / threads).is_integer():\n",
    "            workers = int(cores / threads)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        cluster = LocalCluster(\n",
    "            n_workers=workers,\n",
    "            threads_per_worker=threads,\n",
    "            dashboard_address=':0'\n",
    "        )\n",
    "\n",
    "        print(f\"Started cluster with {workers} workers with {threads} threads\")\n",
    "        print(f\"Dashboard: {cluster.dashboard_link}\")\n",
    "        with Client(cluster) as client:\n",
    "            for i in range(3):\n",
    "                for func in [tasks, arrays, dataframes]:\n",
    "                    print(f\"Iteration {i+1}: {func.__name__}\")\n",
    "                    df = run(func, client=client)\n",
    "                    L.append(df)\n",
    "            print('Computation complete! Stopping workers...')\n",
    "\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6becd2769a0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mddf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pda' is not defined"
     ]
    }
   ],
   "source": [
    "ddf = pda.concat(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf.groupby(['collection', 'name', 'n', 'workers', 'threads', 'unit']).median()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scaling-data-cores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import Row, Column, gridplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(part, axis_type='linear'):\n",
    "    row = part.iloc[0]\n",
    "    title = (row['collection'] + ': ' + row['name']).title().replace('0M', '0m').replace('1S', '1s')\n",
    "\n",
    "    fig = figure(title=title, sizing_mode='scale_width', x_axis_type=axis_type, y_axis_type=axis_type)\n",
    "    fig.line(x=part.threads, y=part.rate)\n",
    "    fig.circle(x=part.threads, y=part.rate)\n",
    "    fig.xaxis.axis_label = 'threads'\n",
    "    fig.yaxis.axis_label = row['unit']\n",
    "    fig.x_range.start = 0\n",
    "    fig.y_range.start = 0\n",
    "\n",
    "    # Add in perfect scaling line\n",
    "#     y_end = fig.y_range.end\n",
    "#     mn = part.n.min()\n",
    "#     mx = part.n.max()\n",
    "#     slope = part[part.n == mn].iloc[0]['rate'] / mn\n",
    "#     fig.line(x=[0, mx], y=[0, slope * mx], color='gray', line_dash='dashed')\n",
    "#     fig.y_range.end = part.rate.max()\n",
    "    \n",
    "    fig.xaxis.ticker = part.threads\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_plot(part, axis_type='linear'):\n",
    "    row = part.iloc[0]\n",
    "    title = (row['collection'] + ': ' + row['name']).title().replace('0M', '0m').replace('1S', '1s')\n",
    "\n",
    "    fig = figure(title=title, sizing_mode='scale_width', x_axis_type=axis_type, y_axis_type=axis_type)\n",
    "    fig.line(x=part.n, y=part.rate)\n",
    "    fig.circle(x=part.n, y=part.rate)\n",
    "    fig.xaxis.axis_label = 'cores'\n",
    "    fig.yaxis.axis_label = row['unit']\n",
    "    fig.x_range.start = 0\n",
    "    fig.y_range.start = 0\n",
    "\n",
    "    # Add in perfect scaling line\n",
    "    y_end = fig.y_range.end\n",
    "    mn = part.n.min()\n",
    "    mx = part.n.max()\n",
    "    slope = part[part.n == mn].iloc[0]['rate'] / mn\n",
    "    fig.line(x=[0, mx], y=[0, slope * mx], color='gray', line_dash='dashed')\n",
    "    fig.y_range.end = part.rate.max()\n",
    "    \n",
    "    fig.xaxis.ticker = part.n\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scaling-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['collection', 'name'])['collection', 'name', 'n', 'rate', 'unit', 'threads'].apply(comparison_plot)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['task map 1s tasks', 'task map 100ms tasks', 'task map fast tasks',\n",
    "         'tree reduction 100ms tasks', 'tree reduction fast tasks', 'sequential',\n",
    "         'nearest neighbor 100ms tasks', 'nearest neighbor fast tasks', \n",
    "         'dynamic tree reduction 100ms tasks', 'dynamic tree reduction fast tasks']\n",
    "from toolz import partition_all\n",
    "L = df2.loc['tasks'].loc[names].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['create random', 'blockwise 100ms tasks', 'elementwise computation', 'reduction', \n",
    "         'reduction along axis', 'random access', 'transpose addition', 'rechunk large', \n",
    "         'nearest neighbor fast tasks', 'nearest neighbor 100ms tasks']\n",
    "from toolz import partition_all\n",
    "L = df2.loc['arrays'].loc[names].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['task map 1s tasks', 'task map 100ms tasks', 'task map fast tasks',\n",
    "         'tree reduction 100ms tasks', 'tree reduction fast tasks', 'sequential',\n",
    "         'nearest neighbor 100ms tasks', 'nearest neighbor fast tasks', \n",
    "         'dynamic tree reduction 100ms tasks', 'dynamic tree reduction fast tasks']\n",
    "L = df2.loc['dataframes'].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(file, method):\n",
    "    if method == 'dask':\n",
    "        t = %timeit -o -n 1 -r 2 dd.read_csv(file + \".csv\")\n",
    "        return t.timings\n",
    "    elif method == 'dask_compute':\n",
    "        t = %timeit -o -n 1 -r 2 dd.read_csv(file + \".csv\").compute()\n",
    "        return t.timings\n",
    "    elif method == 'pandas':\n",
    "        t = %timeit -o -n 1 -r 2 pd.read_csv(file + \".csv\")\n",
    "        return t.timings\n",
    "    else:\n",
    "        return 'Choose an available method'\n",
    "\n",
    "methods = ['dask', 'dask_compute', 'pandas']\n",
    "datasets = ['random', 'random_0.5', 'random_0.1']\n",
    "\n",
    "# benchmark\n",
    "result = {method: [load_df(dataset,method) for dataset in datasets] for method in methods}\n",
    "\n",
    "# Make dataframe\n",
    "df = pd.DataFrame.from_dict(result, orient='index', columns=datasets).reset_index()\n",
    "df.columns = ['method', 'random', 'random_0.5', 'random_0.1']\n",
    "\n",
    "# Make separate columns into one row\n",
    "df = df.melt(id_vars='method', var_name='dataset', value_vars=datasets, value_name='average')\n",
    "\n",
    "# Explode every measurement into separate rows\n",
    "df = df.explode('average')\n",
    "\n",
    "# Plot that shit\n",
    "sns.catplot(data=df, x='dataset', y='average', hue='method', kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dask]",
   "language": "python",
   "name": "conda-env-dask-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
