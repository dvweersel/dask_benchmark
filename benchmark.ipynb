{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the benchmark by Matthew Rocklin (https://matthewrocklin.com/blog/work/2017/07/03/scaling), but expanded to also look at the difference between cores and threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, wait\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(func, client):\n",
    "\n",
    "    client = client or default_client()\n",
    "    \n",
    "    workers = len(client.ncores())\n",
    "    threads = list(client.ncores().values())[0]\n",
    "    n = sum(client.ncores().values())\n",
    "    \n",
    "    coroutine = func(n)\n",
    "\n",
    "    name, unit, numerator = next(coroutine)\n",
    "    out = []\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            next_name, next_unit, next_numerator = next(coroutine)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        finally:\n",
    "            end = time.time()\n",
    "            record = {'name': name, \n",
    "                      'duration': end - start, \n",
    "                      'unit': unit + '/s', \n",
    "                      'rate': numerator / ((end - start) + 1e-10), \n",
    "                      'n': n,\n",
    "                      'workers': workers,\n",
    "                      'threads': threads,\n",
    "                      'collection': func.__name__}\n",
    "            out.append(record)\n",
    "        name = next_name\n",
    "        unit = next_unit\n",
    "        numerator = next_numerator\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import time\n",
    "\n",
    "def slowinc(x, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return x + 1\n",
    "\n",
    "def slowadd(x, y, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return x + y\n",
    "\n",
    "def slowsum(L, delay=0.1):\n",
    "    time.sleep(delay)\n",
    "    return sum(L)\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def tasks(n):\n",
    "    yield 'task map fast tasks', 'tasks', n * 200\n",
    "    \n",
    "    futures = client.map(inc, range(n * 200))\n",
    "    wait(futures)\n",
    "    \n",
    "    yield 'task map 100ms tasks', 'tasks', n * 100\n",
    "\n",
    "    futures = client.map(slowinc, range(100 * n))\n",
    "    wait(futures)\n",
    "        \n",
    "    yield 'task map 1s tasks', 'tasks', n * 4\n",
    "\n",
    "    futures = client.map(slowinc, range(4 * n), delay=1)\n",
    "    wait(futures)\n",
    "\n",
    "    yield 'tree reduction fast tasks', 'tasks', 2**7 * n\n",
    "    \n",
    "    from dask import delayed\n",
    "\n",
    "    L = range(2**7 * n)\n",
    "    while len(L) > 1:\n",
    "        L = list(map(delayed(operator.add), L[0::2], L[1::2]))\n",
    "\n",
    "    L[0].compute()\n",
    "    \n",
    "    yield 'tree reduction 100ms tasks', 'tasks', 2**6 * n * 2\n",
    "    \n",
    "    from dask import delayed\n",
    "\n",
    "    L = range(2**6 * n)\n",
    "    while len(L) > 1:\n",
    "        L = list(map(delayed(slowadd), L[0::2], L[1::2]))\n",
    "\n",
    "    L[0].compute()\n",
    "    \n",
    "    yield 'sequential', 'tasks', 100\n",
    "\n",
    "    x = 1\n",
    "\n",
    "    for i in range(100):\n",
    "        x = delayed(inc)(x)\n",
    "        \n",
    "    x.compute()\n",
    "    \n",
    "    yield 'dynamic tree reduction fast tasks', 'tasks', 100 * n\n",
    "    \n",
    "    from dask.distributed import as_completed\n",
    "    futures = client.map(inc, range(n * 100))\n",
    "    \n",
    "    pool = as_completed(futures)\n",
    "    batches = pool.batches()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batches)\n",
    "            if len(batch) == 1:\n",
    "                batch += next(batches)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        future = client.submit(sum, batch)\n",
    "        pool.add(future)\n",
    "        \n",
    "    yield 'dynamic tree reduction 100ms tasks', 'tasks', 100 * n\n",
    "    \n",
    "    from dask.distributed import as_completed\n",
    "    futures = client.map(slowinc, range(n * 20))\n",
    "    \n",
    "    pool = as_completed(futures)\n",
    "    batches = pool.batches()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(batches)\n",
    "            if len(batch) == 1:\n",
    "                batch += next(batches)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        future = client.submit(slowsum, batch)\n",
    "        pool.add(future)\n",
    "\n",
    "        \n",
    "    yield 'nearest neighbor fast tasks', 'tasks', 100 * n * 2\n",
    "    \n",
    "    L = range(100 * n)\n",
    "    L = client.map(operator.add, L[:-1], L[1:])\n",
    "    L = client.map(operator.add, L[:-1], L[1:])\n",
    "    wait(L)\n",
    "    \n",
    "    yield 'nearest neighbor 100ms tasks', 'tasks', 20 * n * 2\n",
    "    \n",
    "    L = range(20 * n)\n",
    "    L = client.map(slowadd, L[:-1], L[1:])\n",
    "    L = client.map(slowadd, L[:-1], L[1:])\n",
    "    wait(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays(n):\n",
    "    import dask.array as da\n",
    "    N = int(5000 * math.sqrt(n))\n",
    "    x = da.random.randint(0, 10000, size=(N, N), chunks=(10000, 10000))\n",
    "    \n",
    "    yield 'create random', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x = x.persist()\n",
    "    wait(x)\n",
    "    \n",
    "    yield 'blockwise 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_blocks(slowinc, dtype=x.dtype).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'random access', 'bytes', 8\n",
    "    \n",
    "    x[1234, 4567].compute()\n",
    "   \n",
    "    yield 'reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x.std().compute()\n",
    "    \n",
    "    yield 'reduction along axis', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    x.std(axis=0).compute()\n",
    "    \n",
    "    yield 'elementwise computation', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = da.sin(x) ** 2 + da.cos(x) ** 2\n",
    "    y = y.persist()\n",
    "    wait(y)    \n",
    "    \n",
    "    yield 'rechunk small', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.rechunk((20000, 200)).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'rechunk large', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = y.rechunk((200, 20000)).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'transpose addition', 'MB', x.nbytes / 1e6\n",
    "    y = x + x.T\n",
    "    y = y.persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'nearest neighbor fast tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_overlap(inc, depth=1).persist()\n",
    "    wait(y)   \n",
    "        \n",
    "    yield 'nearest neighbor 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = x.map_overlap(slowinc, depth=1, delay=0.1).persist()\n",
    "    wait(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframes(n):\n",
    "    import dask.array as da\n",
    "    import dask.dataframe as dd\n",
    "    N = 2000000 * n\n",
    "    \n",
    "    x = da.random.randint(0, 10000, size=(N, 10), chunks=(10000000, 10))\n",
    "\n",
    "    \n",
    "    yield 'create random', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df = dd.from_dask_array(x).persist()\n",
    "    wait(df)\n",
    "    \n",
    "    yield 'blockwise 100ms tasks', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.map_partitions(slowinc, meta=df).persist())\n",
    "    \n",
    "    yield 'arithmetic', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    y = (df[0] + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10).persist()\n",
    "    wait(y)\n",
    "    \n",
    "    yield 'random access', 'bytes', 8\n",
    "    \n",
    "    df.loc[123456].compute()\n",
    "    \n",
    "    yield 'dataframe reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.std().compute()\n",
    "    \n",
    "    yield 'series reduction', 'MB', x.nbytes / 1e6 / 10\n",
    "    \n",
    "    df[3].std().compute()\n",
    "    \n",
    "    yield 'groupby reduction', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.groupby(0)[1].mean().compute()\n",
    "    \n",
    "    yield 'groupby apply (full shuffle)', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    df.groupby(0).apply(len, meta=('x', 'f8')).compute()\n",
    "    \n",
    "    yield 'set index (full shuffle)', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.set_index(1).persist())\n",
    "    \n",
    "    yield 'rolling aggregations', 'MB', x.nbytes / 1e6\n",
    "    \n",
    "    wait(df.rolling(5).mean().persist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started cluster with 1 workers with 128 threads\n",
      "Dashboard: http://127.0.0.1:36483/status\n",
      "Iteration 1: tasks\n",
      "Iteration 1: arrays\n",
      "Iteration 1: dataframes\n",
      "Iteration 2: tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Future.__del__ at 0x7ff09cbf37b8>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/lib/python3.7/site-packages/distributed/client.py\", line 379, in __del__\n",
      "    self.release()\n",
      "  File \"/opt/anaconda/lib/python3.7/site-packages/distributed/client.py\", line 357, in release\n",
      "    self.client.loop.add_callback(self.client._dec_ref, tokey(self.key))\n",
      "  File \"/opt/anaconda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 176, in add_callback\n",
      "    call_soon(self._run_callback, functools.partial(callback, *args, **kwargs))\n",
      "  File \"/opt/anaconda/lib/python3.7/asyncio/base_events.py\", line 739, in call_soon_threadsafe\n",
      "    self._write_to_self()\n",
      "  File \"/opt/anaconda/lib/python3.7/asyncio/selector_events.py\", line 132, in _write_to_self\n",
      "    csock.send(b'\\0')\n",
      "KeyboardInterrupt: \n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# cores_list = [2, 4, 8, 16, 32, 64]\n",
    "cores_list = [128, 64, 32, 16, 8, 4]\n",
    "threads_list = [4]\n",
    "\n",
    "L = []\n",
    "for cores in cores_list:\n",
    "    for threads in threads_list:\n",
    "        if threads > cores:\n",
    "            break\n",
    "\n",
    "        if (cores / threads).is_integer():\n",
    "            workers = int(cores / threads)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        cluster = LocalCluster(\n",
    "            n_workers=workers,\n",
    "            threads_per_worker=threads,\n",
    "            dashboard_address=':0'\n",
    "        )\n",
    "\n",
    "        print(f\"Started cluster with {workers} workers with {threads} threads\")\n",
    "        print(f\"Dashboard: {cluster.dashboard_link}\")\n",
    "        with Client(cluster) as client:\n",
    "            for i in range(3):\n",
    "                for func in [tasks, arrays, dataframes]:\n",
    "                    print(f\"Iteration {i+1}: {func.__name__}\")\n",
    "                    df = run(func, client=client)\n",
    "                    L.append(df)\n",
    "            print('Computation complete! Stopping workers...')\n",
    "\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.concat(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf.groupby(['collection', 'name', 'n', 'workers', 'threads', 'unit']).median()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scaling-data-cores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import Row, Column, gridplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(part, axis_type='linear'):\n",
    "    row = part.iloc[0]\n",
    "    title = (row['collection'] + ': ' + row['name']).title().replace('0M', '0m').replace('1S', '1s')\n",
    "\n",
    "    fig = figure(title=title, sizing_mode='scale_width', x_axis_type=axis_type, y_axis_type=axis_type)\n",
    "    fig.line(x=part.threads, y=part.rate)\n",
    "    fig.circle(x=part.threads, y=part.rate)\n",
    "    fig.xaxis.axis_label = 'threads'\n",
    "    fig.yaxis.axis_label = row['unit']\n",
    "    fig.x_range.start = 0\n",
    "    fig.y_range.start = 0\n",
    "\n",
    "    # Add in perfect scaling line\n",
    "#     y_end = fig.y_range.end\n",
    "#     mn = part.n.min()\n",
    "#     mx = part.n.max()\n",
    "#     slope = part[part.n == mn].iloc[0]['rate'] / mn\n",
    "#     fig.line(x=[0, mx], y=[0, slope * mx], color='gray', line_dash='dashed')\n",
    "#     fig.y_range.end = part.rate.max()\n",
    "    \n",
    "    fig.xaxis.ticker = part.threads\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_plot(part, axis_type='linear'):\n",
    "    row = part.iloc[0]\n",
    "    title = (row['collection'] + ': ' + row['name']).title().replace('0M', '0m').replace('1S', '1s')\n",
    "\n",
    "    fig = figure(title=title, sizing_mode='scale_width', x_axis_type=axis_type, y_axis_type=axis_type)\n",
    "    fig.line(x=part.n, y=part.rate)\n",
    "    fig.circle(x=part.n, y=part.rate)\n",
    "    fig.xaxis.axis_label = 'cores'\n",
    "    fig.yaxis.axis_label = row['unit']\n",
    "    fig.x_range.start = 0\n",
    "    fig.y_range.start = 0\n",
    "\n",
    "    # Add in perfect scaling line\n",
    "    y_end = fig.y_range.end\n",
    "    mn = part.n.min()\n",
    "    mx = part.n.max()\n",
    "    slope = part[part.n == mn].iloc[0]['rate'] / mn\n",
    "    fig.line(x=[0, mx], y=[0, slope * mx], color='gray', line_dash='dashed')\n",
    "    fig.y_range.end = part.rate.max()\n",
    "    \n",
    "    fig.xaxis.ticker = part.n\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scaling-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['collection', 'name'])['collection', 'name', 'n', 'rate', 'unit', 'threads'].apply(comparison_plot)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['task map 1s tasks', 'task map 100ms tasks', 'task map fast tasks',\n",
    "         'tree reduction 100ms tasks', 'tree reduction fast tasks', 'sequential',\n",
    "         'nearest neighbor 100ms tasks', 'nearest neighbor fast tasks', \n",
    "         'dynamic tree reduction 100ms tasks', 'dynamic tree reduction fast tasks']\n",
    "from toolz import partition_all\n",
    "L = df2.loc['tasks'].loc[names].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['create random', 'blockwise 100ms tasks', 'elementwise computation', 'reduction', \n",
    "         'reduction along axis', 'random access', 'transpose addition', 'rechunk large', \n",
    "         'nearest neighbor fast tasks', 'nearest neighbor 100ms tasks']\n",
    "from toolz import partition_all\n",
    "L = df2.loc['arrays'].loc[names].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['task map 1s tasks', 'task map 100ms tasks', 'task map fast tasks',\n",
    "         'tree reduction 100ms tasks', 'tree reduction fast tasks', 'sequential',\n",
    "         'nearest neighbor 100ms tasks', 'nearest neighbor fast tasks', \n",
    "         'dynamic tree reduction 100ms tasks', 'dynamic tree reduction fast tasks']\n",
    "L = df2.loc['dataframes'].values.tolist()\n",
    "grid = list(partition_all(3, L))\n",
    "# show(Column(*[Row(*g, sizing_mode='scale_width') for g in grid], sizing_mode='scale_width'))\n",
    "show(gridplot(grid, sizing_mode='scale_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clipboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(file, method):\n",
    "    if method == 'dask':\n",
    "        t = %timeit -o -n 1 -r 2 dd.read_csv(file + \".csv\")\n",
    "        return t.timings\n",
    "    elif method == 'dask_compute':\n",
    "        t = %timeit -o -n 1 -r 2 dd.read_csv(file + \".csv\").compute()\n",
    "        return t.timings\n",
    "    elif method == 'pandas':\n",
    "        t = %timeit -o -n 1 -r 2 pd.read_csv(file + \".csv\")\n",
    "        return t.timings\n",
    "    else:\n",
    "        return 'Choose an available method'\n",
    "\n",
    "methods = ['dask', 'dask_compute', 'pandas']\n",
    "datasets = ['random', 'random_0.5', 'random_0.1']\n",
    "\n",
    "# benchmark\n",
    "result = {method: [load_df(dataset,method) for dataset in datasets] for method in methods}\n",
    "\n",
    "# Make dataframe\n",
    "df = pd.DataFrame.from_dict(result, orient='index', columns=datasets).reset_index()\n",
    "df.columns = ['method', 'random', 'random_0.5', 'random_0.1']\n",
    "\n",
    "# Make separate columns into one row\n",
    "df = df.melt(id_vars='method', var_name='dataset', value_vars=datasets, value_name='average')\n",
    "\n",
    "# Explode every measurement into separate rows\n",
    "df = df.explode('average')\n",
    "\n",
    "# Plot that shit\n",
    "sns.catplot(data=df, x='dataset', y='average', hue='method', kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
